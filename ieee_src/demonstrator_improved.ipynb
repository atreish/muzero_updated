{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314ce6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir ./results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87757bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import math\n",
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "import nevergrad\n",
    "import numpy\n",
    "import ray\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "import diagnose_model\n",
    "import models\n",
    "import replay_buffer\n",
    "import self_play\n",
    "import shared_storage\n",
    "import trainer\n",
    "\n",
    "\n",
    "class MuZero:\n",
    "    \"\"\"\n",
    "    Main class to manage MuZero.\n",
    "\n",
    "    Args:\n",
    "        game_name (str): Name of the game module, it should match the name of a .py file\n",
    "        in the \"./games\" directory.\n",
    "\n",
    "        config (dict, MuZeroConfig, optional): Override the default config of the game.\n",
    "\n",
    "        split_resources_in (int, optional): Split the GPU usage when using concurent muzero instances.\n",
    "\n",
    "    Example:\n",
    "        >>> muzero = MuZero(\"cartpole\")\n",
    "        >>> muzero.train()\n",
    "        >>> muzero.test(render=True)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, game_name, config=None, split_resources_in=1):\n",
    "        # Load the game and the config from the module with the game name\n",
    "        try:\n",
    "            game_module = importlib.import_module(\"games.\" + game_name)\n",
    "            self.Game = game_module.Game\n",
    "            self.config = game_module.MuZeroConfig()\n",
    "        except ModuleNotFoundError as err:\n",
    "            print(\n",
    "                f'{game_name} is not a supported game name, try \"cartpole\" or refer to the documentation for adding a new game.'\n",
    "            )\n",
    "            raise err\n",
    "\n",
    "        # Overwrite the config\n",
    "        if config:\n",
    "            if type(config) is dict:\n",
    "                for param, value in config.items():\n",
    "                    setattr(self.config, param, value)\n",
    "            else:\n",
    "                self.config = config\n",
    "\n",
    "        # Fix random generator seed\n",
    "        numpy.random.seed(self.config.seed)\n",
    "        torch.manual_seed(self.config.seed)\n",
    "\n",
    "        # Manage GPUs\n",
    "        total_gpus = (\n",
    "            self.config.max_num_gpus\n",
    "            if self.config.max_num_gpus is not None\n",
    "            else torch.cuda.device_count()\n",
    "        )\n",
    "        self.num_gpus = total_gpus / split_resources_in\n",
    "        if 1 < self.num_gpus:\n",
    "            self.num_gpus = math.floor(self.num_gpus)\n",
    "\n",
    "        ray.init(num_gpus=total_gpus, ignore_reinit_error=True)\n",
    "\n",
    "        # Checkpoint and replay buffer used to initialize workers\n",
    "        self.checkpoint = {\n",
    "            \"weights\": None,\n",
    "            \"optimizer_state\": None,\n",
    "            \"total_reward\": 0,\n",
    "            \"muzero_reward\": 0,\n",
    "            \"opponent_reward\": 0,\n",
    "            \"episode_length\": 0,\n",
    "            \"mean_value\": 0,\n",
    "            \"training_step\": 0,\n",
    "            \"lr\": 0,\n",
    "            \"total_loss\": 0,\n",
    "            \"value_loss\": 0,\n",
    "            \"reward_loss\": 0,\n",
    "            \"policy_loss\": 0,\n",
    "            \"reward_prediction_error\": [],\n",
    "            \"value_prediction_error\": [],\n",
    "            \"num_played_games\": 0,\n",
    "            \"num_played_steps\": 0,\n",
    "            \"num_reanalysed_games\": 0,\n",
    "            \"terminate\": False,\n",
    "        }\n",
    "        self.replay_buffer = {}\n",
    "\n",
    "        # Trick to force DataParallel to stay on CPU\n",
    "        @ray.remote(num_cpus=0, num_gpus=0)\n",
    "        def get_initial_weights(config):\n",
    "            model = models.MuZeroNetwork(config)\n",
    "            weigths = model.get_weights()\n",
    "            summary = str(model).replace(\"\\n\", \" \\n\\n\")\n",
    "            return weigths, summary\n",
    "\n",
    "        self.checkpoint[\"weights\"], self.summary = ray.get(\n",
    "            get_initial_weights.remote(self.config)\n",
    "        )\n",
    "\n",
    "        # Workers\n",
    "        self.self_play_workers = None\n",
    "        self.test_worker = None\n",
    "        self.training_worker = None\n",
    "        self.reanalyse_worker = None\n",
    "        self.replay_buffer_worker = None\n",
    "        self.shared_storage_worker = None\n",
    "\n",
    "    def train(self, log_in_tensorboard=True):\n",
    "        \"\"\"\n",
    "        Spawn ray workers and launch the training.\n",
    "\n",
    "        Args:\n",
    "            log_in_tensorboard (bool): Start a testing worker and log its performance in TensorBoard.\n",
    "        \"\"\"\n",
    "        if log_in_tensorboard or self.config.save_model:\n",
    "            os.makedirs(self.config.results_path, exist_ok=True)\n",
    "\n",
    "        # Manage GPUs\n",
    "        if 0 < self.num_gpus:\n",
    "            num_gpus_per_worker = self.num_gpus / (\n",
    "                self.config.train_on_gpu\n",
    "                + self.config.num_workers * self.config.selfplay_on_gpu\n",
    "                + log_in_tensorboard * self.config.selfplay_on_gpu\n",
    "                + self.config.use_last_model_value * self.config.reanalyse_on_gpu\n",
    "            )\n",
    "            if 1 < num_gpus_per_worker:\n",
    "                num_gpus_per_worker = math.floor(num_gpus_per_worker)\n",
    "        else:\n",
    "            num_gpus_per_worker = 0\n",
    "\n",
    "        # Initialize workers\n",
    "        self.training_worker = trainer.Trainer.options(\n",
    "            num_cpus=0,\n",
    "            num_gpus=num_gpus_per_worker if self.config.train_on_gpu else 0,\n",
    "        ).remote(self.checkpoint, self.config)\n",
    "\n",
    "        self.shared_storage_worker = shared_storage.SharedStorage.remote(\n",
    "            self.checkpoint,\n",
    "            self.config,\n",
    "        )\n",
    "        self.shared_storage_worker.set_info.remote(\"terminate\", False)\n",
    "\n",
    "        self.replay_buffer_worker = replay_buffer.ReplayBuffer.remote(\n",
    "            self.checkpoint, self.replay_buffer, self.config\n",
    "        )\n",
    "\n",
    "        if self.config.use_last_model_value:\n",
    "            self.reanalyse_worker = replay_buffer.Reanalyse.options(\n",
    "                num_cpus=0,\n",
    "                num_gpus=num_gpus_per_worker if self.config.reanalyse_on_gpu else 0,\n",
    "            ).remote(self.checkpoint, self.config)\n",
    "\n",
    "        self.self_play_workers = [\n",
    "            self_play.SelfPlay.options(\n",
    "                num_cpus=0,\n",
    "                num_gpus=num_gpus_per_worker if self.config.selfplay_on_gpu else 0,\n",
    "            ).remote(\n",
    "                self.checkpoint,\n",
    "                self.Game,\n",
    "                self.config,\n",
    "                self.config.seed + seed,\n",
    "            )\n",
    "            for seed in range(self.config.num_workers)\n",
    "        ]\n",
    "\n",
    "        # Launch workers\n",
    "        [\n",
    "            self_play_worker.continuous_self_play.remote(\n",
    "                self.shared_storage_worker, self.replay_buffer_worker\n",
    "            )\n",
    "            for self_play_worker in self.self_play_workers\n",
    "        ]\n",
    "        self.training_worker.continuous_update_weights.remote(\n",
    "            self.replay_buffer_worker, self.shared_storage_worker\n",
    "        )\n",
    "        if self.config.use_last_model_value:\n",
    "            self.reanalyse_worker.reanalyse.remote(\n",
    "                self.replay_buffer_worker, self.shared_storage_worker\n",
    "            )\n",
    "\n",
    "        if log_in_tensorboard:\n",
    "            self.logging_loop(\n",
    "                num_gpus_per_worker if self.config.selfplay_on_gpu else 0,\n",
    "            )\n",
    "\n",
    "    def logging_loop(self, num_gpus):\n",
    "        \"\"\"\n",
    "        Keep track of the training performance.\n",
    "        \"\"\"\n",
    "        # Launch the test worker to get performance metrics\n",
    "        self.test_worker = self_play.SelfPlay.options(\n",
    "            num_cpus=0,\n",
    "            num_gpus=num_gpus,\n",
    "        ).remote(\n",
    "            self.checkpoint,\n",
    "            self.Game,\n",
    "            self.config,\n",
    "            self.config.seed + self.config.num_workers,\n",
    "        )\n",
    "        self.test_worker.continuous_self_play.remote(\n",
    "            self.shared_storage_worker, None, True\n",
    "        )\n",
    "\n",
    "        # Write everything in TensorBoard\n",
    "        writer = SummaryWriter(self.config.results_path)\n",
    "\n",
    "        print(\n",
    "            \"\\nTraining...\\nRun tensorboard --logdir ./results and go to http://localhost:6006/ to see in real time the training performance.\\n\"\n",
    "        )\n",
    "\n",
    "        # Save hyperparameters to TensorBoard\n",
    "        hp_table = [\n",
    "            f\"| {key} | {value} |\" for key, value in self.config.__dict__.items()\n",
    "        ]\n",
    "        writer.add_text(\n",
    "            \"Hyperparameters\",\n",
    "            \"| Parameter | Value |\\n|-------|-------|\\n\" + \"\\n\".join(hp_table),\n",
    "        )\n",
    "        # Save model representation\n",
    "        writer.add_text(\n",
    "            \"Model summary\",\n",
    "            self.summary,\n",
    "        )\n",
    "        # Loop for updating the training performance\n",
    "        counter = 0\n",
    "        keys = [\n",
    "            \"total_reward\",\n",
    "            \"muzero_reward\",\n",
    "            \"opponent_reward\",\n",
    "            \"episode_length\",\n",
    "            \"mean_value\",\n",
    "            \"training_step\",\n",
    "            \"lr\",\n",
    "            \"total_loss\",\n",
    "            \"value_loss\",\n",
    "            \"reward_loss\",\n",
    "            \"policy_loss\",\n",
    "            \"reward_prediction_error\",\n",
    "            \"value_prediction_error\",\n",
    "            \"num_played_games\",\n",
    "            \"num_played_steps\",\n",
    "            \"num_reanalysed_games\",\n",
    "        ]\n",
    "        info = ray.get(self.shared_storage_worker.get_info.remote(keys))\n",
    "        try:\n",
    "            while info[\"training_step\"] < self.config.training_steps:\n",
    "                info = ray.get(self.shared_storage_worker.get_info.remote(keys))\n",
    "                writer.add_scalar(\n",
    "                    \"1.Total reward/1.Total reward\",\n",
    "                    info[\"total_reward\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"1.Total reward/2.Mean value\",\n",
    "                    info[\"mean_value\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"1.Total reward/3.Episode length\",\n",
    "                    info[\"episode_length\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"1.Total reward/4.MuZero reward\",\n",
    "                    info[\"muzero_reward\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"1.Total reward/5.Opponent reward\",\n",
    "                    info[\"opponent_reward\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"2.Workers/1.Self played games\",\n",
    "                    info[\"num_played_games\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"2.Workers/2.Training steps\", info[\"training_step\"], counter\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"2.Workers/3.Self played steps\", info[\"num_played_steps\"], counter\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"2.Workers/4.Reanalysed games\",\n",
    "                    info[\"num_reanalysed_games\"],\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\n",
    "                    \"2.Workers/5.Training steps per self played step ratio\",\n",
    "                    info[\"training_step\"] / max(1, info[\"num_played_steps\"]),\n",
    "                    counter,\n",
    "                )\n",
    "                writer.add_scalar(\"2.Workers/6.Learning rate\", info[\"lr\"], counter)\n",
    "                writer.add_scalar(\n",
    "                    \"3.Loss/1.Total weighted loss\", info[\"total_loss\"], counter\n",
    "                )\n",
    "                writer.add_scalar(\"3.Loss/Value loss\", info[\"value_loss\"], counter)\n",
    "                writer.add_scalar(\"3.Loss/Reward loss\", info[\"reward_loss\"], counter)\n",
    "                writer.add_scalar(\"3.Loss/Policy loss\", info[\"policy_loss\"], counter)\n",
    "\n",
    "                for i, (error_reward, error_value) in enumerate(zip(info[\"reward_prediction_error\"], info[\"value_prediction_error\"])):\n",
    "                    writer.add_scalar(\"4.Prediction errors/Reward/Step {}\".format(i + 1), error_reward, counter)\n",
    "                    writer.add_scalar(\"4.Prediction errors/Value/Step {}\".format(i + 1), error_value, counter)\n",
    "\n",
    "                print(\n",
    "                    f'Last test reward: {info[\"total_reward\"]:.2f}. Training step: {info[\"training_step\"]}/{self.config.training_steps}. Played games: {info[\"num_played_games\"]}. Loss: {info[\"total_loss\"]:.2f}',\n",
    "                    end=\"\\r\",\n",
    "                )\n",
    "                counter += 1\n",
    "                time.sleep(0.5)\n",
    "        except KeyboardInterrupt:\n",
    "            pass\n",
    "\n",
    "        self.terminate_workers()\n",
    "\n",
    "        if self.config.save_model:\n",
    "            # Persist replay buffer to disk\n",
    "            print(\"\\n\\nPersisting replay buffer games to disk...\")\n",
    "            pickle.dump(\n",
    "                self.replay_buffer,\n",
    "                open(os.path.join(self.config.results_path, \"replay_buffer.pkl\"), \"wb\"),\n",
    "            )\n",
    "\n",
    "    def terminate_workers(self):\n",
    "        \"\"\"\n",
    "        Softly terminate the running tasks and garbage collect the workers.\n",
    "        \"\"\"\n",
    "        if self.shared_storage_worker:\n",
    "            self.shared_storage_worker.set_info.remote(\"terminate\", True)\n",
    "            self.checkpoint = ray.get(\n",
    "                self.shared_storage_worker.get_checkpoint.remote()\n",
    "            )\n",
    "        if self.replay_buffer_worker:\n",
    "            self.replay_buffer = ray.get(self.replay_buffer_worker.get_buffer.remote())\n",
    "\n",
    "        print(\"\\nShutting down workers...\")\n",
    "\n",
    "        self.self_play_workers = None\n",
    "        self.test_worker = None\n",
    "        self.training_worker = None\n",
    "        self.reanalyse_worker = None\n",
    "        self.replay_buffer_worker = None\n",
    "        self.shared_storage_worker = None\n",
    "\n",
    "    def test(\n",
    "        self, render=True, opponent=None, muzero_player=None, num_tests=1, num_gpus=0\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Test the model in a dedicated thread.\n",
    "\n",
    "        Args:\n",
    "            render (bool): To display or not the environment. Defaults to True.\n",
    "\n",
    "            opponent (str): \"self\" for self-play, \"human\" for playing against MuZero and \"random\"\n",
    "            for a random agent, None will use the opponent in the config. Defaults to None.\n",
    "\n",
    "            muzero_player (int): Player number of MuZero in case of multiplayer\n",
    "            games, None let MuZero play all players turn by turn, None will use muzero_player in\n",
    "            the config. Defaults to None.\n",
    "\n",
    "            num_tests (int): Number of games to average. Defaults to 1.\n",
    "\n",
    "            num_gpus (int): Number of GPUs to use, 0 forces to use the CPU. Defaults to 0.\n",
    "        \"\"\"\n",
    "        opponent = opponent if opponent else self.config.opponent\n",
    "        muzero_player = muzero_player if muzero_player else self.config.muzero_player\n",
    "        self_play_worker = self_play.SelfPlay.options(\n",
    "            num_cpus=0,\n",
    "            num_gpus=num_gpus,\n",
    "        ).remote(self.checkpoint, self.Game, self.config, numpy.random.randint(10000))\n",
    "        results = []\n",
    "        for i in range(num_tests):\n",
    "            print(f\"Testing {i+1}/{num_tests}\")\n",
    "            results.append(\n",
    "                ray.get(\n",
    "                    self_play_worker.play_game.remote(\n",
    "                        0,\n",
    "                        0,\n",
    "                        render,\n",
    "                        opponent,\n",
    "                        muzero_player,\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        self_play_worker.close_game.remote()\n",
    "\n",
    "        if len(self.config.players) == 1:\n",
    "            result = numpy.mean([sum(history.reward_history) for history in results])\n",
    "        else:\n",
    "            result = numpy.mean(\n",
    "                [\n",
    "                    sum(\n",
    "                        reward\n",
    "                        for i, reward in enumerate(history.reward_history)\n",
    "                        if history.to_play_history[i - 1] == muzero_player\n",
    "                    )\n",
    "                    for history in results\n",
    "                ]\n",
    "            )\n",
    "        return result\n",
    "\n",
    "    def load_model(self, checkpoint_path=None, replay_buffer_path=None):\n",
    "        \"\"\"\n",
    "        Load a model and/or a saved replay buffer.\n",
    "\n",
    "        Args:\n",
    "            checkpoint_path (str): Path to model.checkpoint or model.weights.\n",
    "\n",
    "            replay_buffer_path (str): Path to replay_buffer.pkl\n",
    "        \"\"\"\n",
    "        # Load checkpoint\n",
    "        if checkpoint_path:\n",
    "            if os.path.exists(checkpoint_path):\n",
    "                self.checkpoint = torch.load(checkpoint_path)\n",
    "                print(f\"\\nUsing checkpoint from {checkpoint_path}\")\n",
    "            else:\n",
    "                print(f\"\\nThere is no model saved in {checkpoint_path}.\")\n",
    "\n",
    "        # Load replay buffer\n",
    "        if replay_buffer_path:\n",
    "            if os.path.exists(replay_buffer_path):\n",
    "                with open(replay_buffer_path, \"rb\") as f:\n",
    "                    self.replay_buffer = pickle.load(f)\n",
    "                print(f\"\\nInitializing replay buffer with {replay_buffer_path}\")\n",
    "            else:\n",
    "                print(\n",
    "                    f\"Warning: Replay buffer path '{replay_buffer_path}' doesn't exist.  Using empty buffer.\"\n",
    "                )\n",
    "                self.checkpoint[\"training_step\"] = 0\n",
    "                self.checkpoint[\"num_played_steps\"] = 0\n",
    "                self.checkpoint[\"num_played_games\"] = 0\n",
    "                self.checkpoint[\"num_reanalysed_games\"] = 0\n",
    "\n",
    "    def diagnose_model(self, horizon):\n",
    "        \"\"\"\n",
    "        Play a game only with the learned model then play the same trajectory in the real\n",
    "        environment and display information.\n",
    "\n",
    "        Args:\n",
    "            horizon (int): Number of timesteps for which we collect information.\n",
    "        \"\"\"\n",
    "        game = self.Game(self.config.seed)\n",
    "        obs = game.reset()\n",
    "        dm = diagnose_model.DiagnoseModel(self.checkpoint, self.config)\n",
    "        dm.compare_virtual_with_real_trajectories(obs, game, horizon)\n",
    "        input(\"Press enter to close all plots\")\n",
    "        dm.close_all()\n",
    "\n",
    "\n",
    "def hyperparameter_search(\n",
    "    game_name, parametrization, budget, parallel_experiments, num_tests\n",
    "):\n",
    "    \"\"\"\n",
    "    Search for hyperparameters by launching parallel experiments.\n",
    "\n",
    "    Args:\n",
    "        game_name (str): Name of the game module, it should match the name of a .py file\n",
    "        in the \"./games\" directory.\n",
    "\n",
    "        parametrization : Nevergrad parametrization, please refer to nevergrad documentation.\n",
    "\n",
    "        budget (int): Number of experience to launch in total.\n",
    "\n",
    "        parallel_experiments (int): Number of experience to launch in parallel.\n",
    "\n",
    "        num_tests (int): Number of games to average for evaluating an experiment.\n",
    "    \"\"\"\n",
    "    optimizer = nevergrad.optimizers.OnePlusOne(\n",
    "        parametrization=parametrization, budget=budget\n",
    "    )\n",
    "\n",
    "    running_experiments = []\n",
    "    best_training = None\n",
    "    try:\n",
    "        # Launch initial experiments\n",
    "        for i in range(parallel_experiments):\n",
    "            if 0 < budget:\n",
    "                param = optimizer.ask()\n",
    "                print(f\"Launching new experiment: {param.value}\")\n",
    "                muzero = MuZero(game_name, param.value, parallel_experiments)\n",
    "                muzero.param = param\n",
    "                muzero.train(False)\n",
    "                running_experiments.append(muzero)\n",
    "                budget -= 1\n",
    "\n",
    "        while 0 < budget or any(running_experiments):\n",
    "            for i, experiment in enumerate(running_experiments):\n",
    "                if experiment and experiment.config.training_steps <= ray.get(\n",
    "                    experiment.shared_storage_worker.get_info.remote(\"training_step\")\n",
    "                ):\n",
    "                    experiment.terminate_workers()\n",
    "                    result = experiment.test(False, num_tests=num_tests)\n",
    "                    if not best_training or best_training[\"result\"] < result:\n",
    "                        best_training = {\n",
    "                            \"result\": result,\n",
    "                            \"config\": experiment.config,\n",
    "                            \"checkpoint\": experiment.checkpoint,\n",
    "                        }\n",
    "                    print(f\"Parameters: {experiment.param.value}\")\n",
    "                    print(f\"Result: {result}\")\n",
    "                    optimizer.tell(experiment.param, -result)\n",
    "\n",
    "                    if 0 < budget:\n",
    "                        param = optimizer.ask()\n",
    "                        print(f\"Launching new experiment: {param.value}\")\n",
    "                        muzero = MuZero(game_name, param.value, parallel_experiments)\n",
    "                        muzero.param = param\n",
    "                        muzero.train(False)\n",
    "                        running_experiments[i] = muzero\n",
    "                        budget -= 1\n",
    "                    else:\n",
    "                        running_experiments[i] = None\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        for experiment in running_experiments:\n",
    "            if isinstance(experiment, MuZero):\n",
    "                experiment.terminate_workers()\n",
    "\n",
    "    recommendation = optimizer.provide_recommendation()\n",
    "    print(\"Best hyperparameters:\")\n",
    "    print(recommendation.value)\n",
    "    if best_training:\n",
    "        # Save best training weights (but it's not the recommended weights)\n",
    "        os.makedirs(best_training[\"config\"].results_path, exist_ok=True)\n",
    "        torch.save(\n",
    "            best_training[\"checkpoint\"],\n",
    "            os.path.join(best_training[\"config\"].results_path, \"model.checkpoint\"),\n",
    "        )\n",
    "        # Save the recommended hyperparameters\n",
    "        text_file = open(\n",
    "            os.path.join(best_training[\"config\"].results_path, \"best_parameters.txt\"),\n",
    "            \"w\",\n",
    "        )\n",
    "        text_file.write(str(recommendation.value))\n",
    "        text_file.close()\n",
    "    return recommendation.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20943ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nWelcome to MuZero! Here's a list of games:\")\n",
    "# Let user pick a game\n",
    "games = [\n",
    "    filename.stem\n",
    "    for filename in sorted(list((pathlib.Path.cwd() / \"games\").glob(\"*.py\")))\n",
    "    if filename.name != \"abstract_game.py\"\n",
    "]\n",
    "for i in range(len(games)):\n",
    "    print(f\"{i}. {games[i]}\")\n",
    "choice = input(\"Enter a number to choose the game: \")\n",
    "valid_inputs = [str(i) for i in range(len(games))]\n",
    "while choice not in valid_inputs:\n",
    "    choice = input(\"Invalid input, enter a number listed above: \")\n",
    "\n",
    "# Initialize MuZero\n",
    "choice = int(choice)\n",
    "game_name = games[choice]\n",
    "muzero = MuZero(game_name)\n",
    "\n",
    "while True:\n",
    "    # Configure running options\n",
    "    options = [\n",
    "        \"Train\",\n",
    "        \"Load pretrained model\",\n",
    "        \"Diagnose model\",\n",
    "        \"Render some self play games\",\n",
    "        \"Play against MuZero\",\n",
    "        \"Test the game manually\",\n",
    "        \"Hyperparameter search\",\n",
    "        \"Exit\",\n",
    "    ]\n",
    "    print()\n",
    "    for i in range(len(options)):\n",
    "        print(f\"{i}. {options[i]}\")\n",
    "\n",
    "    choice = input(\"Enter a number to choose an action: \")\n",
    "    valid_inputs = [str(i) for i in range(len(options))]\n",
    "    while choice not in valid_inputs:\n",
    "        choice = input(\"Invalid input, enter a number listed above: \")\n",
    "    choice = int(choice)\n",
    "    if choice == 0:\n",
    "        muzero.train()\n",
    "    elif choice == 1:\n",
    "        checkpoint_path = input(\n",
    "            \"Enter a path to the model.checkpoint, or ENTER if none: \"\n",
    "        )\n",
    "        while checkpoint_path and not os.path.isfile(checkpoint_path):\n",
    "            checkpoint_path = input(\"Invalid checkpoint path. Try again: \")\n",
    "        replay_buffer_path = input(\n",
    "            \"Enter a path to the replay_buffer.pkl, or ENTER if none: \"\n",
    "        )\n",
    "        while replay_buffer_path and not os.path.isfile(replay_buffer_path):\n",
    "            replay_buffer_path = input(\n",
    "                \"Invalid replay buffer path. Try again: \"\n",
    "            )\n",
    "        muzero.load_model(\n",
    "            checkpoint_path=checkpoint_path,\n",
    "            replay_buffer_path=replay_buffer_path,\n",
    "        )\n",
    "    elif choice == 2:\n",
    "        muzero.diagnose_model(30)\n",
    "    elif choice == 3:\n",
    "        muzero.test(render=True, opponent=\"self\", muzero_player=None)\n",
    "    elif choice == 4:\n",
    "        muzero.test(render=True, opponent=\"human\", muzero_player=0)\n",
    "    elif choice == 5:\n",
    "        env = muzero.Game()\n",
    "        env.reset()\n",
    "        env.render()\n",
    "\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = env.human_to_action()\n",
    "            observation, reward, done = env.step(action)\n",
    "            print(f\"\\nAction: {env.action_to_string(action)}\\nReward: {reward}\")\n",
    "            env.render()\n",
    "    elif choice == 6:\n",
    "        # Define here the parameters to tune\n",
    "        # Parametrization documentation: https://facebookresearch.github.io/nevergrad/parametrization.html\n",
    "        muzero.terminate_workers()\n",
    "        del muzero\n",
    "        budget = 20\n",
    "        parallel_experiments = 2\n",
    "        lr_init = nevergrad.p.Log(a_min=0.0001, a_max=0.1)\n",
    "        discount = nevergrad.p.Log(lower=0.95, upper=0.9999)\n",
    "        parametrization = nevergrad.p.Dict(\n",
    "            lr_init=lr_init, discount=discount\n",
    "        )\n",
    "        best_hyperparameters = hyperparameter_search(\n",
    "            game_name, parametrization, budget, parallel_experiments, 20\n",
    "        )\n",
    "        muzero = MuZero(game_name, best_hyperparameters)\n",
    "    else:\n",
    "        break\n",
    "    print(\"\\nDone\")\n",
    "\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
